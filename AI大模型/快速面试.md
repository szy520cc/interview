以下是关于**大语言模型（LLM）**的核心知识点总结，结合面试高频题和实际应用场景，帮助您系统掌握关键概念：

---

### **一、LLM 核心原理**
1. **本质**：基于概率的文本生成模型，通过神经网络预测下一个Token的概率分布。
2. **工作流程**：
   - 输入文本 → Token化 → 模型计算概率分布 → 选择策略（采样/贪心）→ 生成输出。

---

### **二、模型请求配置参数**
| **参数**          | **作用**                          | **典型值**       | **场景**                     |
|-------------------|----------------------------------|------------------|-----------------------------|
| **Temperature**   | 控制输出的随机性                 | 0（确定）~2（创意） | 低值：代码生成；高值：创意写作 |
| **Top_p**         | 限制候选Token范围（累积概率）     | 0.5（精准）~0.95（多样） | 结合Temperature精细控制输出   |
| **Max_tokens**    | **输入+输出的总Token数限制**     | 100~2000+        | 必须预留余量防截断           |
| **Stop**          | 设置停止词提前终止生成            | e.g. `["\n"]`    | 控制输出长度或格式           |
| **Presence_penalty** | 惩罚已出现过的Token（防重复）    | 0~2              | 降低内容重复率               |
| **Frequency_penalty** | 惩罚高频出现的Token             | 0~2              | 抑制重复短语                 |

> ✅ **关键细节**：`Max_tokens`包括输入和输出的总和；低`Temperature+低Top_p`可减少幻觉（Hallucination）。

---

### **三、提示词设计规范**
#### **优秀提示词 = 清晰指令 + 结构化输出 + 上下文**
1. **反例**：“写后端开发内容。” → 输出宽泛  
2. **正例**：  
   ```markdown
   任务：生成用户注册API文档  
   要求：
   - 方法：POST  
   - URL：/api/register  
   - 参数：username(字符串,必填), password(字符串,必填)  
   - 响应：{code:200, data:{user_id:123}}  
   - 错误码：400（参数缺失）  
   ```
3. **结构化输出优势**：  
   - 后端直接解析JSON/XML，便于自动化集成。

---

### **四、高频优化策略**
#### **问题：输出重复**
- **方案**：  
  `Temperature↓` + `Presence_penalty↑` + 提示词加“避免重复内容”。

#### **问题：响应延迟**
- **后端优化**：  
  ```mermaid
  graph LR
    A[请求] --> B{缓存？}
    B -->|是| C[返回缓存结果]
    B -->|否| D[异步调用LLM]
    D --> E[消息队列批量处理]
    E --> F[就近节点部署]
  ```

---

### **五、Agent vs 普通LLM调用**
| **特性**         | 普通LLM调用               | Agent系统                |
|------------------|--------------------------|--------------------------|
| **执行模式**     | 单次响应                  | 多步骤规划+工具调用      |
| **核心能力**     | 文本生成                  | 任务分解/记忆/自我修正   |
| **后端集成模块** | 无                        | 规划+工具调用+记忆+反馈  |
| **应用场景**     | Q&A/摘要                  | 自动运维/数据分析流水线  |

> ✅ **示例**：Agent监控日志 → 发现异常 → 调用重启脚本 → 验证结果 → 通知管理员。

---

### **六、关键技术拓展**
#### 1. **上下文窗口（Context Window）**
- **定义**：模型单次处理的**输入+输出Token上限**（如128K）。  
- **超限处理**：  
  - **分块（Chunking）**：按语义拆分文本分批处理。  
  - **摘要（Summarization）**：压缩历史信息。

#### 2. **微调（Fine-tuning）**
- **适用场景**：  
  - 领域专业化（医疗/法律）  
  - 强制输出格式（公司规范JSON）  
  - **小样本高精度任务**（需数千条标注数据）。

#### 3. **RAG（检索增强生成）**
```mermaid
  sequenceDiagram
    用户->>后端： 查询“最新财报”
    后端->>向量数据库： 检索相似文档
    向量数据库-->>后端： 返回Top 3片段
    后端->>LLM： 提示词+检索结果
    LLM-->>后端： 生成基于事实的答案
    后端-->>用户： 返回答案
```
> **优势**：融合实时数据，减少幻觉。

---

### **七、安全与合规**
1. **API调用风险**：  
   - **加密传输**：强制HTTPS  
   - **输入过滤**：防御提示词注入（Prompt Injection）  
   - **输出消毒**：移除敏感信息/PII数据  
2. **合规要求**：  
   - 数据存储地域（如GDPR）  
   - 审计日志记录  

---

### **面试点睛题**
1. **如何降低LLM的幻觉？**  
   - **答**：`Temperature=0.3` + `Top_p=0.5` + RAG检索增强 + 提示词约束（“仅基于提供事实回答”）。  

2. **Agent任务失败如何设计重试？**  
   - **答**：反馈模块捕获错误 → 规划模块调整子任务 → 熔断机制避兔死循环。  

3. **长文本如何处理128K上下文限制？**  
   - **答**：滑动窗口分块 + 增量摘要（保留核心信息）。  

---

> 掌握以上内容可覆盖90% LLM面试考点，重点理解**参数交互**、**Agent架构**及**RAG实战逻辑**。